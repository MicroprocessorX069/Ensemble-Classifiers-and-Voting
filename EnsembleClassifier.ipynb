{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EnsembleClassifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MicroprocessorX069/Ensemble-Classifiers-and-Voting/blob/master/EnsembleClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "5NH0esHyj9Yj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load MNIST on Python 3.x"
      ]
    },
    {
      "metadata": {
        "id": "uoqFcXrrj9Yl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import gzip\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xwNXqpPgj9Yr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filename = 'mnist.pkl.gz'\n",
        "f = gzip.open(filename, 'rb')\n",
        "training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EUB6OykMouOF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Driver"
      ]
    },
    {
      "metadata": {
        "id": "mNtPHshhpL6d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2701026f-d684-44c8-ce21-8967b7540bd5"
      },
      "cell_type": "code",
      "source": [
        "trainData=np.array(training_data[0])\n",
        "trainResult=np.array(training_data[1])\n",
        "validationData=np.array(validation_data[0])\n",
        "validationResult=np.array(validation_data[1])\n",
        "testData=np.array(test_data[0])\n",
        "testResult=np.array(test_data[1])\n",
        "Data=np.concatenate((trainData,validationData,testData),axis=0)\n",
        "print(Data.shape)\n",
        "Result=np.concatenate((trainResult,validationResult,testResult),axis=0)\n",
        "print(Result.shape)\n",
        "from sklearn.cluster import KMeans\n",
        "learningRate=0.01\n",
        "batchSize=10000\n",
        "train_split=0.9\n",
        "validation_split=0\n",
        "no_basisFn=8\n",
        "features_concat=18\n",
        "features_subtract=9\n",
        "m=10\n",
        "IsSynthetic=False\n",
        "\n",
        "\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(70000, 784)\n",
            "(70000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X2ABol-Aa8_t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#SVM\n",
        "Kernels:\n",
        "Kernels are used to transform non separable data to a higher dimensions where it might become separable.\n",
        "\n",
        "Types of kernels:\n"
      ]
    },
    {
      "metadata": {
        "id": "F4UXl_S5YWpl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "from sklearn.svm import SVC \n",
        "def SVM(kernelName='rbf'):\n",
        "  svclassifier = SVC(kernel=kernelName)  \n",
        "  svclassifier.fit(trainData, trainResult) \n",
        "  return svclassifier\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zWx0bFUIbOd7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Testing data"
      ]
    },
    {
      "metadata": {
        "id": "5Jbodm-FbC3s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "testPrediction = svclassifier.predict(testData)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GuvKbk5GbRsL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Evaluation and confusion matrix"
      ]
    },
    {
      "metadata": {
        "id": "-4bgwzFLbNJ9",
        "colab_type": "code",
        "outputId": "e702ec46-d312-4e9b-e29b-b65d304fd071",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix  \n",
        "import pandas as pd\n",
        "print(pd.DataFrame(confusion_matrix(testResult,testPrediction)))  \n",
        "print(classification_report(testResult,testPrediction))\n",
        "print(\"Time taken:\", svm_time)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     0     1    2    3    4    5    6    7    8    9\n",
            "0  967     0    1    0    0    5    4    1    2    0\n",
            "1    0  1120    2    3    0    1    3    1    5    0\n",
            "2    9     1  962    7   10    1   13   11   16    2\n",
            "3    1     1   14  950    1   17    1   10   11    4\n",
            "4    1     1    7    0  937    0    7    2    2   25\n",
            "5    7     4    5   33    7  808   11    2   10    5\n",
            "6   10     3    4    1    5   10  924    0    1    0\n",
            "7    2    13   22    5    7    1    0  954    4   20\n",
            "8    4     6    6   14    8   24   10    8  891    3\n",
            "9   10     6    0   12   33    5    1   14    6  922\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "          0       0.96      0.99      0.97       980\n",
            "          1       0.97      0.99      0.98      1135\n",
            "          2       0.94      0.93      0.94      1032\n",
            "          3       0.93      0.94      0.93      1010\n",
            "          4       0.93      0.95      0.94       982\n",
            "          5       0.93      0.91      0.92       892\n",
            "          6       0.95      0.96      0.96       958\n",
            "          7       0.95      0.93      0.94      1028\n",
            "          8       0.94      0.91      0.93       974\n",
            "          9       0.94      0.91      0.93      1009\n",
            "\n",
            "avg / total       0.94      0.94      0.94     10000\n",
            "\n",
            "Time taken: 805.9282991886139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KjV9ZOtNb46A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Citation\n",
        "https://stackabuse.com/implementing-svm-and-kernel-svm-with-pythons-scikit-learn/"
      ]
    },
    {
      "metadata": {
        "id": "yZo9PeSyxJnH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Random Forest\n",
        "Random forest is a combination of decision tree. Where each tree makes a decision and votes for a class. The algorithm then chooses the most popular class as the result. RF can do both, classification and regression. More dense the forest, robust is the process(more accurate).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "vrrOL5Cvx7ka",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import time\n",
        "def RandomForest():\n",
        "  start=time.time()\n",
        "  #creating random forest classifier\n",
        "  #n_estimators are no. of voting decision trees in the forest. More the trees better the final result\n",
        "  #criterion can be gini or entropy for each of the decision tree and to evaluate the final best split\n",
        "  #max_features are the no. of features to be looked while evaluating the best split\n",
        "  #min_samples_leaf doesnot allow any leaf to have less than spcified no. of data samples.\n",
        "\n",
        "  clf=RandomForestClassifier(n_estimators=100,min_samples_leaf=500)\n",
        "\n",
        "  #Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "  clf.fit(trainData,trainResult)\n",
        "  end=time.time()\n",
        "  print(\"Time taken:\",end-start)\n",
        "  return clf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "29BVCQAwyjy7",
        "colab_type": "code",
        "outputId": "169ba250-7a89-46bc-d678-d4cd4336f652",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix  \n",
        "from sklearn import metrics\n",
        "import pandas as pd\n",
        "\n",
        "print(pd.DataFrame(confusion_matrix(testResult,testPrediction)))  \n",
        "print(classification_report(testResult,testPrediction))\n",
        "print(\"Accuracy:\",metrics.accuracy_score(testResult, testPrediction))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     0     1     2    3    4    5    6    7    8    9\n",
            "0  970     1     1    0    0    1    4    1    2    0\n",
            "1    0  1124     2    3    0    1    3    1    1    0\n",
            "2    6     0  1003    5    2    0    2    8    6    0\n",
            "3    0     0     8  977    0    5    0    9    7    4\n",
            "4    1     0     3    0  954    0    4    0    2   18\n",
            "5    4     0     1   13    3  858    5    1    5    2\n",
            "6    7     3     1    0    2    3  938    0    4    0\n",
            "7    1     3    20    2    1    0    0  988    3   10\n",
            "8    3     0     6    6    5    5    5    4  930   10\n",
            "9    4     5     2   12   12    3    1    4    6  960\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "          0       0.97      0.99      0.98       980\n",
            "          1       0.99      0.99      0.99      1135\n",
            "          2       0.96      0.97      0.96      1032\n",
            "          3       0.96      0.97      0.96      1010\n",
            "          4       0.97      0.97      0.97       982\n",
            "          5       0.98      0.96      0.97       892\n",
            "          6       0.98      0.98      0.98       958\n",
            "          7       0.97      0.96      0.97      1028\n",
            "          8       0.96      0.95      0.96       974\n",
            "          9       0.96      0.95      0.95      1009\n",
            "\n",
            "avg / total       0.97      0.97      0.97     10000\n",
            "\n",
            "Accuracy: 0.9702\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rxvQQg6i1nKc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finding the important features.\n",
        "\n",
        "Random forest also weighs each features according to their importance or contribution in the final result. Here we print the list of top 10 important features because the no of features for this dataset it large"
      ]
    },
    {
      "metadata": {
        "id": "JyETBpUf1_km",
        "colab_type": "code",
        "outputId": "07c2d2f2-13c5-462f-ec51-477cf2675e9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "feature_imp = pd.Series(clf.feature_importances_).sort_values(ascending=False)\n",
        "#printing top 10 featurs with their resp. importance\n",
        "feature_imp.iloc[0:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "378    0.010522\n",
              "489    0.008883\n",
              "406    0.008294\n",
              "433    0.008112\n",
              "350    0.008030\n",
              "211    0.007641\n",
              "347    0.007358\n",
              "154    0.007104\n",
              "405    0.006837\n",
              "461    0.006812\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "boo5kPUm0eGX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Citations\n",
        "https://www.datacamp.com/community/tutorials/random-forests-classifier-python\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"
      ]
    },
    {
      "metadata": {
        "id": "0Ggd8BqKbAL0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Logistic Regression"
      ]
    },
    {
      "metadata": {
        "id": "0-iatJ0ARYmO",
        "colab_type": "code",
        "outputId": "2eab0ae6-3c1c-4c38-c90a-972b351263c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "trainData=np.array(trainData[0:1000])\n",
        "trainResult=np.array(trainResult[0:1000])\n",
        "trainTranspose=np.transpose(trainData)\n",
        "kmeans=KMeans(m,random_state=0).fit(trainData)\n",
        "means=np.array(kmeans.cluster_centers_)\n",
        "variance_mat     = GenerateBigSigma(trainTranspose, means,100 ,IsSynthetic) # generating variance for each input feature\n",
        "train_phi = GetPhiMatrix(trainTranspose, means, variance_mat, 100)\n",
        "print(\"Training Data shape: \",train_phi.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Data shape:  (1000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "O_bhdSMPRZYz",
        "colab_type": "code",
        "outputId": "437056f2-cf3a-445d-e51f-6e985c9e3289",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "testData=np.array(testData[0:100])\n",
        "testResult=np.array(testResult[0:100])\n",
        "testTranspose=np.transpose(testData)\n",
        "kmeans=KMeans(m,random_state=0).fit(testData)\n",
        "means=np.array(kmeans.cluster_centers_)\n",
        "#ClusterIndicesNumpy(7, kmeans.labels_)\n",
        "variance_mat     = GenerateBigSigma(testTranspose, means,100 ,IsSynthetic) # generating variance for each input feature\n",
        "test_phi = GetPhiMatrix(testTranspose, means, variance_mat, 100)\n",
        "print(\"Test Data shape: \",test_phi.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Data shape:  (100, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uKaeKeD2RcNL",
        "colab_type": "code",
        "outputId": "143399c7-14b0-4609-8d33-d52b3e9b72f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size=50\n",
        "loss_arr=grad_descent(train_phi,trainResult,test_phi, testResult,batch_size)\n",
        "loss_arr"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6931471805599455,\n",
              " -3.604289487507272,\n",
              " -7.227602256275578,\n",
              " -10.185136355305229,\n",
              " -13.047307987598002,\n",
              " -15.754478927131807,\n",
              " -18.95540074275045,\n",
              " -22.360428303902992,\n",
              " -26.104982430951043,\n",
              " -29.527273352181545,\n",
              " -32.38707615600333,\n",
              " -36.290894044410884,\n",
              " -39.994182336639135,\n",
              " -43.77358408656323,\n",
              " -47.11786555893557,\n",
              " -50.30932885000976,\n",
              " -53.698477191522905,\n",
              " -56.20809484717717,\n",
              " -59.46050431747936,\n",
              " -62.464223693926236]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "kAC1y28SorJ8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Functions"
      ]
    },
    {
      "metadata": {
        "id": "pdWW7rLQopjP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_dataset(Dataset,no_features,train_percent=0.8,validation_percent=0.1,test_percent=0.1):\n",
        "  train_start=0\n",
        "  train_end=train_start+int(Dataset.shape[0]*train_percent)\n",
        "  train_data=Dataset[train_start:train_end,0:no_features]\n",
        "  train_result=Dataset[train_start:train_end,no_features]\n",
        "  \n",
        "  validation_start=train_end+1\n",
        "  validation_end=validation_start+int(Dataset.shape[0]*validation_percent)\n",
        "  validation_data=Dataset[validation_start:validation_end,0:no_features]\n",
        "  validation_result=Dataset[validation_start:validation_end,no_features]\n",
        "  \n",
        "  test_start=validation_end+1\n",
        "  test_end=test_start+int(Dataset.shape[0]*test_percent)\n",
        "  test_data=Dataset[test_start:test_end,0:no_features]\n",
        "  test_result=Dataset[test_start:test_end,no_features]\n",
        "  \n",
        "  return train_data, train_result, validation_data, validation_result, test_data, test_result\n",
        "\n",
        "def createDataset(df,train_percent, validation_percent):\n",
        "  df=df.loc[:,'Af1':'target']\n",
        "  length=df.shape[0]\n",
        "  \n",
        "  trainEnd=int(length*train_percent)\n",
        "  \n",
        "  validationEnd=trainEnd+int(length*validation_percent)\n",
        "  \n",
        "  trainData=df.iloc[0:trainEnd]\n",
        "  trainResult=trainData.loc[:,'target']\n",
        "  trainData=trainData.loc[:,'Af1':'Bf9']\n",
        "  trainResult=trainResult.reset_index(drop=True)\n",
        "  trainData=trainData.reset_index(drop=True)\n",
        "  \n",
        "  validationData=df.iloc[trainEnd:validationEnd]\n",
        "  validationResult=validationData.loc[:,'target']\n",
        "  validationData=validationData.loc[:,'Af1':'Bf9']\n",
        "  validationResult=validationResult.reset_index(drop=True)\n",
        "  validationData=validationData.reset_index(drop=True)\n",
        "  \n",
        "  testData=df.iloc[validationEnd:length]\n",
        "  testResult=testData.loc[:,'target']\n",
        "  testData=testData.loc[:,'Af1':'Bf9']\n",
        "  testResult=testResult.reset_index(drop=True)\n",
        "  testData=testData.reset_index(drop=True)\n",
        "  \n",
        "  return trainData, trainResult, validationData, validationResult, testData, testResult\n",
        "  \n",
        "def clusterIndices(no_clusters, labels_array): #numpy \n",
        "  for cluster_no in range(no_clusters):\n",
        "    \n",
        "   return np.where(labels_array == no_clusters)[0]\n",
        "\n",
        "def GenerateBigSigma(Data, MuMatrix,TrainingPercent,IsSynthetic):\n",
        "  # To calculate BigSigma as\n",
        "  #BigSigma 41 x 41\n",
        "  #Matrix of variances between input features, to calculate PhiMatrix\n",
        "    '''\n",
        "                          x(1)      x(2)      x(3) .  .  .     x(41)\n",
        "    BigSigma =   x(1)  Var(1,1)      0         0                 0\n",
        "\n",
        "                 x(2)       0      Var(2,2)    0                 0\n",
        "\n",
        "                 x(3)       0       0        Var(3,3)            0\n",
        "                  .\n",
        "                  .\n",
        "                  .\n",
        "                 x(41)                                      Var(41,41)\n",
        "      '''        \n",
        "    \n",
        "    #Initializing BigSigma matrix as zeros since we just have to populate the diagonal elements\n",
        "    \n",
        "    BigSigma    = np.zeros((len(Data),len(Data))) #shape = 41,41\n",
        "    DataT       = np.transpose(Data) #len(DataT)=69k \n",
        "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01)) #math ceil return smallest integer less than X, basically converting to nearest bigger int #Len of training data set = 80% of 69k ~ 67k \n",
        "    varVect     = []\n",
        "    \n",
        "    # Calculating the variance for each input feature, based on training data. np.var return variance of an array\n",
        "    for i in range(0,len(DataT[0])):\n",
        "        vct = []\n",
        "        for j in range(0,int(TrainingLen)):\n",
        "            vct.append(Data[i][j])    \n",
        "        varVect.append(np.var(vct)) #Varianance of Data[i,;] i.e. input feature i\n",
        "    \n",
        "    #Populating BigSigma Matrix\n",
        "    for j in range(len(Data)):\n",
        "        BigSigma[j][j] = varVect[j] ## All other indices other than j,j (i.e. where i != j) would be zero since the covariance between two input features is no required.\n",
        "    if IsSynthetic == True:         # Use of isSynthetic not known yet\n",
        "        BigSigma = np.dot(3,BigSigma)\n",
        "    else:\n",
        "        BigSigma = np.dot(200,BigSigma)\n",
        "    ##print (\"BigSigma Generated..\")\n",
        "    return BigSigma\n",
        "\n",
        "def GetScalar(DataRow,MuRow, BigSigInv):  \n",
        "    #Converts the matrices' (x-Mu), BigSigmaInverse, (x-Mu) product to scalar\n",
        "    \n",
        "    #R is (x-Mu); Shape : 1 x 41\n",
        "    R = np.subtract(DataRow,MuRow)\n",
        "    #np.transpose(R) is Transpose of (x-Mu); Shape :  41 x 1\n",
        "    # Shape of BigSigInv is 41 x 41\n",
        "    # Therefore Shape of T i.e. (R x BigSigmaInverse x Inv(R)): (41) x 41) . (41 x 1)\n",
        "    #                                                          : 41 x 1 \n",
        "    T = np.dot(BigSigInv,np.transpose(R))  \n",
        "    L = np.dot(R,T) #(R . T)\n",
        "    # Therefore Shape of T i.e. (R x BigSigmaInverse x Inv(R)): (1 x 41). (41) x 41) . (41 x 1)\n",
        "    #                                                          : 1 x 1 # HenceScalaar\n",
        "    return L\n",
        "\n",
        "def GetRadialBasisOut(DataRow,MuRow, BigSigInv):  \n",
        "  #Calculation of Phi(x)\n",
        "  #Vector Phi(x) = (-0.5 . (x-Mu) . BigSigmaInverse . (x-Mu))\n",
        "   #              e\n",
        "    # math.exp is exponential function e^(x) where x is the argument of the function\n",
        "    phi_x = math.exp(-0.5*GetScalar(DataRow,MuRow,BigSigInv))\n",
        "    return phi_x\n",
        "\n",
        "def GetPhiMatrix(Data, MuMatrix, BigSigma, TrainingPercent = 80):\n",
        "    #Phi Matrix Shape:  No. of basis fns x Dataset_height i.e. 10 x 67k\n",
        "    # 10 Basis function matrix applied on 41 input feautres\n",
        "    '''\n",
        "    #Phi(x(1)(1/10)) Means Basis function of 1/10 th part of input feature 1 (We divided the input features with kmeans below into 10 clusters)\n",
        "    \n",
        "    Vector Phi(x) = (-0.5 . (x-Mu) . BigSigmaInverse . (x-Mu))\n",
        "                   e\n",
        "                   \n",
        "                   \n",
        "                        Phi1                   Phi2                     Phi3                    .  .  .     Phi10    # no of basis functions i.e. 10\n",
        "  PhiMatrix=   x(1)   Phi(x(1)(1/10))      Phi(x(1)(2/10))         Phi(x(1)(3/10))     .  .  .        Phi(x(1)(10/10))\n",
        "  \n",
        "               x(2)   Phi(x(2)(1/10))      Phi(x(2)(2/10))         Phi(x(2)(3/10))     .  .  .        Phi(x(2)(10/10))\n",
        "    \n",
        "               x(3)   Phi(x(3)(1/10))      Phi(x(3)(2/10))         Phi(x(3)(3/10))     .  .  .        Phi(x(3)(10/10))\n",
        "                .\n",
        "                .\n",
        "                .\n",
        "              x(67k)  Phi(x(67k)(1/10))    Phi(x(67k)(2/10))       Phi(x(67k)(3/10))     .  .  .      Phi(x(67k)(1/10))\n",
        "    '''        \n",
        "    DataT = np.transpose(Data)\n",
        "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))         \n",
        "    PHI = np.zeros((int(TrainingLen),len(MuMatrix))) \n",
        "    \n",
        "    # Calculating inverse of BigSigma matrix for the formula of phi\n",
        "    BigSigInv = np.linalg.pinv(BigSigma)\n",
        "    for  C in range(0,len(MuMatrix)):\n",
        "        for R in range(0,int(TrainingLen)):\n",
        "            #Calculating Phi of each x1\n",
        "            PHI[R][C] = GetRadialBasisOut(DataT[R], MuMatrix[C], BigSigInv)\n",
        "    #print (\"PHI Generated..\")\n",
        "    return PHI\n",
        "  \n",
        "import math\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + math.exp(-x))\n",
        "\n",
        "  \n",
        "def getOutput(input_data, weights):\n",
        "  predicted_result=[]\n",
        "  for input in input_data:\n",
        "      output=0\n",
        "      for i in range(1,len(weights)):\n",
        "        output=output+weights[i]*input[i-1]\n",
        "      output=sigmoid(output)\n",
        "      predicted_result.append(output)\n",
        "  return(np.array(predicted_result))\n",
        "\n",
        "def updateWeights(input_data,target_result,predicted_result,prev_weights,learning_rate):\n",
        "  subtract=np.subtract(predicted_result,target_result)\n",
        "  weight_change=np.dot(subtract,input_data)\n",
        "  weight_change/=(input_data.shape[1])\n",
        "  weight_change*=learning_rate\n",
        "  new_weights=np.subtract(prev_weights,weight_change)\n",
        "  return new_weights\n",
        "\n",
        "def loss(target_result,predicted_result):\n",
        "  loss=-target_result*np.log(predicted_result)-(1-target_result)*np.log(1-predicted_result)\n",
        "  loss=loss.sum()/target_result.shape[0]\n",
        "  return loss\n",
        "  \n",
        "def grad_descent(train_phi,trainResult,validation_phi,validationResult,batch_size,learning_rate=0.1):\n",
        "  W=np.zeros(train_phi.shape[1])\n",
        "  i=0\n",
        "  loss_arr=[]\n",
        "  while i+batch_size <= train_phi.shape[0]:\n",
        "    input_data=train_phi[i:i+batch_size]\n",
        "    target_result=trainResult[i:i+batch_size]\n",
        "    i=i+batch_size\n",
        "    new_lr=learning_rate-(1/i)\n",
        "    if(new_lr<=0):\n",
        "      new_lr=learning_rate\n",
        "    predicted_result=getOutput(input_data,W)\n",
        "    validation_predictedResult=getOutput(validation_phi,W)\n",
        "    W=updateWeights(input_data,target_result,predicted_result,W,learning_rate)\n",
        "    loss_arr.append(loss(validationResult,validation_predictedResult))\n",
        "  return(loss_arr)   \n",
        "\n",
        "def gradDescentLogistic(training_data, validation_data, testing_data ,feature_no, train_percent=0.9,validation_percent=0,m=8,IsSynthetic=False,batch_size=20000,learning_rate=0.01):\n",
        "  training_input=np.array(training_data[0])\n",
        "  training_target=np.array(training_data[1])\n",
        "  validation_input=np.array(validation_data[0])\n",
        "  validation_target=np.array(validation_data[1])\n",
        "  test_input=np.array(test_data[0])\n",
        "  test_target=np.array(test_data[1])\n",
        "  trainData, trainResult, validationData, validationResult, testData, testResult = create_dataset(Dataset,feature_no,train_percent, validation_percent)\n",
        "  trainData=np.array(trainData)\n",
        "  trainResult=np.array(trainResult)\n",
        "  testData=np.array(testData)\n",
        "  testResult=np.array(testResult)\n",
        "\n",
        "  from sklearn.cluster import KMeans\n",
        "\n",
        "  trainTranspose=np.transpose(trainData)\n",
        "  kmeans=KMeans(m,random_state=0).fit(trainData)\n",
        "  means=np.array(kmeans.cluster_centers_)\n",
        "  variance_mat     = GenerateBigSigma(trainTranspose, means,100 ,IsSynthetic) # generating variance for each input feature\n",
        "  train_phi = GetPhiMatrix(trainTranspose, means, variance_mat, 100)\n",
        "  print(\"Training Data shape: \",train_phi.shape)\n",
        "\n",
        "  '''validationTranspose=np.transpose(validationData)\n",
        "  kmeans=KMeans(m,random_state=0).fit(validationData)\n",
        "  means=np.array(kmeans.cluster_centers_)\n",
        "  #ClusterIndicesNumpy(7, kmeans.labels_)\n",
        "  variance_mat     = GenerateBigSigma(validationTranspose, means,100 ,IsSynthetic) # generating variance for each input feature\n",
        "  validation_phi = GetPhiMatrix(validationTranspose, means, variance_mat, 100)\n",
        "  print(\"Validation Data shape: \",train_phi.shape)\n",
        "  '''\n",
        "  testTranspose=np.transpose(testData)\n",
        "  kmeans=KMeans(m,random_state=0).fit(testData)\n",
        "  means=np.array(kmeans.cluster_centers_)\n",
        "  #ClusterIndicesNumpy(7, kmeans.labels_)\n",
        "  variance_mat     = GenerateBigSigma(testTranspose, means,100 ,IsSynthetic) # generating variance for each input feature\n",
        "  test_phi = GetPhiMatrix(testTranspose, means, variance_mat, 100)\n",
        "  print(\"Test Data shape: \",test_phi.shape)\n",
        "  loss_arr=grad_descent(train_phi,trainResult,test_phi, testResult,batch_size)\n",
        "  return loss_arr\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nx5k-GNvj9Yx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load USPS on Python 3.x"
      ]
    },
    {
      "metadata": {
        "id": "_1HNCpafj9Yz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SSSbTr1-j9Y4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "USPSMat  = []\n",
        "USPSTar  = []\n",
        "curPath  = 'USPSdata/Numerals'\n",
        "savedImg = []\n",
        "\n",
        "for j in range(0,10):\n",
        "    curFolderPath = curPath + '/' + str(j)\n",
        "    imgs =  os.listdir(curFolderPath)\n",
        "    for img in imgs:\n",
        "        curImg = curFolderPath + '/' + img\n",
        "        if curImg[-3:] == 'png':\n",
        "            img = Image.open(curImg,'r')\n",
        "            img = img.resize((28, 28))\n",
        "            savedImg = img\n",
        "            imgdata = (255-np.array(img.getdata()))/255\n",
        "            USPSMat.append(imgdata)\n",
        "            USPSTar.append(j)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iYjPimss37gA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "cXXZaoJQ3_uh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Boosting\n",
        "\n",
        "Boosting is a technique based on ensemble. It learns on multiple algorithms and weighs its samples which are wrongly classified in the previous algorithm."
      ]
    },
    {
      "metadata": {
        "id": "1YDgkTfm4CE_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5f8ed3ef-8bfb-462f-bff2-328feedf9727"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix  \n",
        "from sklearn import metrics\n",
        "from sklearn import cross_validation\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import pandas as pd\n",
        "RandomForest=RandomForestClassifier()\n",
        "kernel='rbf'\n",
        "SVM=SVC(kernel='rbf')\n",
        "for clf,label in zip([RandomForest,SVM],[\"Random Forest Classifier\",\"Support Vector Classifier\"]):\n",
        "  accuracy=cross_validation.cross_val_score(clf,Data,Result,cv=5,scoring='accuracy')\n",
        "  print(\"Accuracy for \",label,\": \",accuracy.mean())\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for  Random Forest Classifier :  0.9446284188836727\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NcFNRpR-LVB5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.base import ClassifierMixin\n",
        "import numpy as np\n",
        "import operator\n",
        "\n",
        "class EnsembleClassifier(BaseEstimator,ClassifierMixin):\n",
        "  def __init__(self,clfs,weights=None):\n",
        "    self.clfs=clfs\n",
        "    self.weights=weights\n",
        "  \n",
        "  def fit(self, train_data, train_result):\n",
        "    for clf in self.clfs:\n",
        "      clf.fit(train_data,train_result)\n",
        "  \n",
        "  def predict(self, train_data):\n",
        "    self.prediction=np.asarray([clf.predict(train_data) for clf in self.clfs])\n",
        "    if self.weights:\n",
        "      avg=self.predict_proba(train_data)\n",
        "      maj=np.apply_along_axis(lambda x:max(enumerate(x),key=operator.itemgetter(1))[0],axis=1,arr=avg)\n",
        "    else:\n",
        "      maj=np.asarray([np.argmax(np.bitcount(self.prediction[:,c])) for c in range(self.prediction.shape[1])])\n",
        "    return maj\n",
        "  \n",
        "  def predict_proba(self,train_data):\n",
        "    self.probabs=[clf.predict_proba(train_data) for clf in self.clfs]\n",
        "    avg=np.average(self.probabs,axis=0,weights=self.weights)\n",
        "    return avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aCKxL5U8Ot2L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(3546)\n",
        "enClf=EnsembleClassifier(clf=[RandomForest,SVM],weights=[1,1])\n",
        "accuracy=cross_validation.cross_val_score(enClf,Data,Result,cv=5,scoring='accuracy')\n",
        "print(\"Accuracy for Ensemble Classifier: \",accuracy.mean())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HI7jaBbd85wr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Citation\n",
        "https://sebastianraschka.com/Articles/2014_ensemble_classifier.html"
      ]
    }
  ]
}