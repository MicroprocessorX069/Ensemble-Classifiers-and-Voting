{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EnsembleClassifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MicroprocessorX069/Ensemble-Classifiers-and-Voting/blob/master/EnsembleClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "5NH0esHyj9Yj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load MNIST on Python 3.x"
      ]
    },
    {
      "metadata": {
        "id": "uoqFcXrrj9Yl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import gzip\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xwNXqpPgj9Yr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filename = 'mnist.pkl.gz'\n",
        "f = gzip.open(filename, 'rb')\n",
        "training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EUB6OykMouOF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Driver"
      ]
    },
    {
      "metadata": {
        "id": "mNtPHshhpL6d",
        "colab_type": "code",
        "outputId": "0cceed15-a31b-47ec-ab9e-67b8ccf02f1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        "trainData=np.array(training_data[0])\n",
        "trainResult=np.array(training_data[1])\n",
        "validationData=np.array(validation_data[0])\n",
        "validationResult=np.array(validation_data[1])\n",
        "testData=np.array(test_data[0])\n",
        "testResult=np.array(test_data[1])\n",
        "Data=np.concatenate((trainData,validationData,testData),axis=0)\n",
        "\n",
        "print(Data.shape)\n",
        "Result=np.concatenate((trainResult,validationResult,testResult),axis=0)\n",
        "print(Result.shape)\n",
        "from sklearn.cluster import KMeans\n",
        "learningRate=0.01\n",
        "batchSize=10000\n",
        "train_split=0.9\n",
        "validation_split=0\n",
        "no_basisFn=8\n",
        "features_concat=18\n",
        "features_subtract=9\n",
        "m=10\n",
        "IsSynthetic=False\n",
        "\n",
        "#Data Contraction\n",
        "temp=np.array([training_data[1]])\n",
        "print(training_data[0].shape,temp.shape)\n",
        "smallTrain=(np.concatenate((training_data[0],temp.T),axis=1))\n",
        "np.random.shuffle(smallTrain)\n",
        "smallTrain=smallTrain[0:5000]\n",
        "print(smallTrain.shape)\n",
        "smallTrainData=smallTrain[:,0:784]\n",
        "smallTrainResult=smallTrain[:,784]\n",
        "\n",
        "temp=np.array([validation_data[1]])\n",
        "print(validation_data[0].shape,temp.shape)\n",
        "smallValid=(np.concatenate((validation_data[0],temp.T),axis=1))\n",
        "np.random.shuffle(smallValid)\n",
        "smallValid=smallValid[0:500]\n",
        "print(smallValid.shape)\n",
        "smallValidData=smallValid[:,0:784]\n",
        "smallValidResult=smallValid[:,784]\n",
        "\n",
        "temp=np.array([test_data[1]])\n",
        "print(test_data[0].shape,temp.shape)\n",
        "smallTest=(np.concatenate((test_data[0],temp.T),axis=1))\n",
        "np.random.shuffle(smallTest)\n",
        "smallTest=smallTest[0:500]\n",
        "print(smallTest.shape)\n",
        "smallTestData=smallTest[:,0:784]\n",
        "smallTestResult=smallTest[:,784]\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "learningRate=0.01\n",
        "batchSize=10000\n",
        "train_split=0.9\n",
        "validation_split=0\n",
        "no_basisFn=8\n",
        "features_concat=18\n",
        "features_subtract=9\n",
        "m=10\n",
        "IsSynthetic=False\n",
        "\n",
        "#Actual Data\n",
        "# smallTrainData=trainData\n",
        "# smallTrainResult=trainResult\n",
        "# smallValidData=validationData\n",
        "# smallValidResult=validationResult\n",
        "# smallTestData=testData\n",
        "# smallTestResult=testResult\n",
        "\n",
        "trainData=smallTrainData\n",
        "trainResult=(smallTrainResult)\n",
        "validData=smallValidData\n",
        "validResult=(smallValidResult)\n",
        "testData=smallTestData\n",
        "testResultInt=smallTestResult\n",
        "testResult=(smallTestResult)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(70000, 784)\n",
            "(70000,)\n",
            "(50000, 784) (1, 50000)\n",
            "(5000, 785)\n",
            "(10000, 784) (1, 10000)\n",
            "(500, 785)\n",
            "(10000, 784) (1, 10000)\n",
            "(500, 785)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X2ABol-Aa8_t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#SVM\n",
        "Kernels:\n",
        "Kernels are used to transform non separable data to a higher dimensions where it might become separable.\n",
        "\n",
        "Types of kernels:\n"
      ]
    },
    {
      "metadata": {
        "id": "F4UXl_S5YWpl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC \n",
        "def SVM(kernelName='rbf'):\n",
        "  svclassifier = SVC(kernel=kernelName)  \n",
        "  svclassifier.fit(trainData, trainResult) \n",
        "  return svclassifier\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zWx0bFUIbOd7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Testing data"
      ]
    },
    {
      "metadata": {
        "id": "5Jbodm-FbC3s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "testPrediction_svm = SVM().predict(testData)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GuvKbk5GbRsL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Evaluation and confusion matrix"
      ]
    },
    {
      "metadata": {
        "id": "-4bgwzFLbNJ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix  \n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "print(pd.DataFrame(confusion_matrix(testResult,testPrediction_svm)))  \n",
        "print(classification_report(testResult,testPrediction_svm))\n",
        "print(\"Accuracy:\",metrics.accuracy_score(testResult, testPrediction_svm))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6VUK3wHHFF3u",
        "colab_type": "code",
        "outputId": "edb5a454-5b8d-4a31-f508-da8f52773363",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "USPStestPrediction_svm=SVM().predict(USPSMat)\n",
        "print(confusion_matrix(USPSTar,USPStestPrediction_svm))\n",
        "print(\"Accuracy:\",metrics.accuracy_score(USPSTar, USPStestPrediction_svm))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 502    8  433   12  405  249  113   43   15  220]\n",
            " [  77  568  144  106  373  228   38  437   16   13]\n",
            " [ 136   54 1182   52   63  286   93   94   31    8]\n",
            " [  85   13  148  941   29  615   11   83   40   35]\n",
            " [  13  163   57    8 1159  292   17  153   62   76]\n",
            " [ 101   35  168   70   37 1434   71   60   16    8]\n",
            " [ 245   24  333   24  141  477  702   18   12   24]\n",
            " [  59  445  383  101   82  442   23  399   38   28]\n",
            " [  62   54  176  119  142 1018   98   40  259   32]\n",
            " [  31  311  216  183  249  206   10  485  177  132]]\n",
            "Accuracy: 0.3639181959097955\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KjV9ZOtNb46A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Citation\n",
        "https://stackabuse.com/implementing-svm-and-kernel-svm-with-pythons-scikit-learn/"
      ]
    },
    {
      "metadata": {
        "id": "yZo9PeSyxJnH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Random Forest\n",
        "Random forest is a combination of decision tree. Where each tree makes a decision and votes for a class. The algorithm then chooses the most popular class as the result. RF can do both, classification and regression. More dense the forest, robust is the process(more accurate).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "vrrOL5Cvx7ka",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import time\n",
        "def RandomForest():\n",
        "  \n",
        "  #creating random forest classifier\n",
        "  #n_estimators are no. of voting decision trees in the forest. More the trees better the final result\n",
        "  #criterion can be gini or entropy for each of the decision tree and to evaluate the final best split\n",
        "  #max_features are the no. of features to be looked while evaluating the best split\n",
        "  #min_samples_leaf doesnot allow any leaf to have less than spcified no. of data samples.\n",
        "\n",
        "  clf=RandomForestClassifier(n_estimators=100,min_samples_leaf=500)\n",
        "\n",
        "  #Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "  clf.fit(trainData,trainResult)\n",
        "  return clf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "29BVCQAwyjy7",
        "colab_type": "code",
        "outputId": "472ed2d9-ba4c-4e9b-ed61-3aa220cf3e08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix  \n",
        "from sklearn import metrics\n",
        "import pandas as pd\n",
        "\n",
        "rf=RandomForest()\n",
        "testPrediction_rf=rf.predict(testData)\n",
        "print(pd.DataFrame(confusion_matrix(testResult,testPrediction_rf)))  \n",
        "print(classification_report(testResult,testPrediction_rf))\n",
        "print(\"Accuracy:\",metrics.accuracy_score(testResult, testPrediction_rf))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0   1   2   3   4  5   6   7   8   9\n",
            "0  37   0   1   0   0  0   0   0   0   0\n",
            "1   0  66   0   0   0  0   0   0   0   0\n",
            "2   4  14  27   1   0  0   1   4   0   0\n",
            "3   9   7   0  27   0  1   0   2   1   3\n",
            "4   0   1   0   0  34  0   1   1   0   8\n",
            "5  15   5   0  11   2  1   1   0   3   9\n",
            "6   6   3   0   1   1  0  41   1   0   1\n",
            "7   1   3   1   0   0  0   0  47   0   0\n",
            "8   2  19   2   4   0  0   0   0  19   4\n",
            "9   1   2   0   1   1  0   1   3   1  37\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "        0.0       0.49      0.97      0.65        38\n",
            "        1.0       0.55      1.00      0.71        66\n",
            "        2.0       0.87      0.53      0.66        51\n",
            "        3.0       0.60      0.54      0.57        50\n",
            "        4.0       0.89      0.76      0.82        45\n",
            "        5.0       0.50      0.02      0.04        47\n",
            "        6.0       0.91      0.76      0.83        54\n",
            "        7.0       0.81      0.90      0.85        52\n",
            "        8.0       0.79      0.38      0.51        50\n",
            "        9.0       0.60      0.79      0.68        47\n",
            "\n",
            "avg / total       0.70      0.67      0.64       500\n",
            "\n",
            "Accuracy: 0.672\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U2eciUFmEIIS",
        "colab_type": "code",
        "outputId": "84ce287d-f4f9-421d-e1bf-f911dcecaced",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "USPStestPrediction_rf=rf.predict(USPSMat)\n",
        "print(confusion_matrix(USPSTar,USPStestPrediction_rf))\n",
        "print(\"Accuracy:\",metrics.accuracy_score(USPSTar, USPStestPrediction_rf))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 686  658    3    2   21    0    0  599    0   31]\n",
            " [  84  999    4   21    2    0    2  886    1    1]\n",
            " [ 255 1121   38   10    3    0    1  570    0    1]\n",
            " [ 315 1260    6   57   10    0    2  339    1   10]\n",
            " [ 105 1155    1    9   69    0    0  651    0   10]\n",
            " [ 566  971    3   20   15    2    8  399    1   15]\n",
            " [ 784  701   12    2    7    0   15  477    0    2]\n",
            " [  92 1330    1    1    0    0    0  575    0    1]\n",
            " [ 476 1269    6   29   17    0    7  192    1    3]\n",
            " [ 128 1098   10   23    8    0    0  730    0    3]]\n",
            "Accuracy: 0.12225611280564028\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rxvQQg6i1nKc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finding the important features.\n",
        "\n",
        "Random forest also weighs each features according to their importance or contribution in the final result. Here we print the list of top 10 important features because the no of features for this dataset it large"
      ]
    },
    {
      "metadata": {
        "id": "JyETBpUf1_km",
        "colab_type": "code",
        "outputId": "07c2d2f2-13c5-462f-ec51-477cf2675e9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "feature_imp = pd.Series(clf.feature_importances_).sort_values(ascending=False)\n",
        "#printing top 10 featurs with their resp. importance\n",
        "feature_imp.iloc[0:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "378    0.010522\n",
              "489    0.008883\n",
              "406    0.008294\n",
              "433    0.008112\n",
              "350    0.008030\n",
              "211    0.007641\n",
              "347    0.007358\n",
              "154    0.007104\n",
              "405    0.006837\n",
              "461    0.006812\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "boo5kPUm0eGX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Citations\n",
        "https://www.datacamp.com/community/tutorials/random-forests-classifier-python\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"
      ]
    },
    {
      "metadata": {
        "id": "0Ggd8BqKbAL0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Logistic Regression"
      ]
    },
    {
      "metadata": {
        "id": "kAC1y28SorJ8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Functions"
      ]
    },
    {
      "metadata": {
        "id": "pdWW7rLQopjP",
        "colab_type": "code",
        "outputId": "ff375d51-26b7-412b-a4d2-0f5c48aa9432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "def clusterIndices(no_clusters, labels_array): #numpy \n",
        "  for cluster_no in range(no_clusters):\n",
        "    \n",
        "   return np.where(labels_array == no_clusters)[0]\n",
        "\n",
        "def GenerateBigSigma(Data, MuMatrix,TrainingPercent,IsSynthetic):\n",
        "  # To calculate BigSigma as\n",
        "  #BigSigma 41 x 41\n",
        "  #Matrix of variances between input features, to calculate PhiMatrix\n",
        "    '''\n",
        "                          x(1)      x(2)      x(3) .  .  .     x(41)\n",
        "    BigSigma =   x(1)  Var(1,1)      0         0                 0\n",
        "\n",
        "                 x(2)       0      Var(2,2)    0                 0\n",
        "\n",
        "                 x(3)       0       0        Var(3,3)            0\n",
        "                  .\n",
        "                  .\n",
        "                  .\n",
        "                 x(41)                                      Var(41,41)\n",
        "      '''        \n",
        "    \n",
        "    #Initializing BigSigma matrix as zeros since we just have to populate the diagonal elements\n",
        "    \n",
        "    BigSigma    = np.zeros((len(Data),len(Data))) #shape = 41,41\n",
        "    DataT       = np.transpose(Data) #len(DataT)=69k \n",
        "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01)) #math ceil return smallest integer less than X, basically converting to nearest bigger int #Len of training data set = 80% of 69k ~ 67k \n",
        "    varVect     = []\n",
        "    \n",
        "    # Calculating the variance for each input feature, based on training data. np.var return variance of an array\n",
        "    for i in range(0,len(DataT[0])):\n",
        "        vct = []\n",
        "        for j in range(0,int(TrainingLen)):\n",
        "            vct.append(Data[i][j])    \n",
        "        varVect.append(np.var(vct)) #Varianance of Data[i,;] i.e. input feature i\n",
        "    \n",
        "    #Populating BigSigma Matrix\n",
        "    for j in range(len(Data)):\n",
        "        BigSigma[j][j] = varVect[j] ## All other indices other than j,j (i.e. where i != j) would be zero since the covariance between two input features is no required.\n",
        "    if IsSynthetic == True:         # Use of isSynthetic not known yet\n",
        "        BigSigma = np.dot(3,BigSigma)\n",
        "    else:\n",
        "        BigSigma = np.dot(200,BigSigma)\n",
        "    ##print (\"BigSigma Generated..\")\n",
        "    return BigSigma\n",
        "\n",
        "def GetScalar(DataRow,MuRow, BigSigInv):  \n",
        "    #Converts the matrices' (x-Mu), BigSigmaInverse, (x-Mu) product to scalar\n",
        "    \n",
        "    #R is (x-Mu); Shape : 1 x 41\n",
        "    R = np.subtract(DataRow,MuRow)\n",
        "    #np.transpose(R) is Transpose of (x-Mu); Shape :  41 x 1\n",
        "    # Shape of BigSigInv is 41 x 41\n",
        "    # Therefore Shape of T i.e. (R x BigSigmaInverse x Inv(R)): (41) x 41) . (41 x 1)\n",
        "    #                                                          : 41 x 1 \n",
        "    T = np.dot(BigSigInv,np.transpose(R))  \n",
        "    L = np.dot(R,T) #(R . T)\n",
        "    # Therefore Shape of T i.e. (R x BigSigmaInverse x Inv(R)): (1 x 41). (41) x 41) . (41 x 1)\n",
        "    #                                                          : 1 x 1 # HenceScalaar\n",
        "    return L\n",
        "\n",
        "def GetRadialBasisOut(DataRow,MuRow, BigSigInv):  \n",
        "  #Calculation of Phi(x)\n",
        "  #Vector Phi(x) = (-0.5 . (x-Mu) . BigSigmaInverse . (x-Mu))\n",
        "   #              e\n",
        "    # math.exp is exponential function e^(x) where x is the argument of the function\n",
        "    phi_x = math.exp(-0.5*GetScalar(DataRow,MuRow,BigSigInv))\n",
        "    return phi_x\n",
        "\n",
        "def GetPhiMatrix(Data, MuMatrix, BigSigma, TrainingPercent = 80):\n",
        "    #Phi Matrix Shape:  No. of basis fns x Dataset_height i.e. 10 x 67k\n",
        "    # 10 Basis function matrix applied on 41 input feautres\n",
        "    '''\n",
        "    #Phi(x(1)(1/10)) Means Basis function of 1/10 th part of input feature 1 (We divided the input features with kmeans below into 10 clusters)\n",
        "    \n",
        "    Vector Phi(x) = (-0.5 . (x-Mu) . BigSigmaInverse . (x-Mu))\n",
        "                   e\n",
        "                   \n",
        "                   \n",
        "                        Phi1                   Phi2                     Phi3                    .  .  .     Phi10    # no of basis functions i.e. 10\n",
        "  PhiMatrix=   x(1)   Phi(x(1)(1/10))      Phi(x(1)(2/10))         Phi(x(1)(3/10))     .  .  .        Phi(x(1)(10/10))\n",
        "  \n",
        "               x(2)   Phi(x(2)(1/10))      Phi(x(2)(2/10))         Phi(x(2)(3/10))     .  .  .        Phi(x(2)(10/10))\n",
        "    \n",
        "               x(3)   Phi(x(3)(1/10))      Phi(x(3)(2/10))         Phi(x(3)(3/10))     .  .  .        Phi(x(3)(10/10))\n",
        "                .\n",
        "                .\n",
        "                .\n",
        "              x(67k)  Phi(x(67k)(1/10))    Phi(x(67k)(2/10))       Phi(x(67k)(3/10))     .  .  .      Phi(x(67k)(1/10))\n",
        "    '''        \n",
        "    DataT = np.transpose(Data)\n",
        "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))         \n",
        "    PHI = np.zeros((int(TrainingLen),len(MuMatrix))) \n",
        "    \n",
        "    # Calculating inverse of BigSigma matrix for the formula of phi\n",
        "    BigSigInv = np.linalg.pinv(BigSigma)\n",
        "    for  C in range(0,len(MuMatrix)):\n",
        "        for R in range(0,int(TrainingLen)):\n",
        "            #Calculating Phi of each x1\n",
        "            PHI[R][C] = GetRadialBasisOut(DataT[R], MuMatrix[C], BigSigInv)\n",
        "    #print (\"PHI Generated..\")\n",
        "    return PHI\n",
        "  \n",
        "from keras.utils import np_utils\n",
        "\n",
        "def encodeOutput(y):\n",
        "  processed_y=[]\n",
        "                                                                                 \n",
        "  for y_label in y:\n",
        "    if(y_label==0):\n",
        "      processed_y.append([0])\n",
        "    elif (y_label==1):\n",
        "      processed_y.append([1])\n",
        "    elif (y_label==2):\n",
        "      processed_y.append([2])\n",
        "    elif (y_label==3):\n",
        "      processed_y.append([3])\n",
        "    elif (y_label==4):\n",
        "      processed_y.append([4])\n",
        "    elif (y_label==5):\n",
        "      processed_y.append([5])\n",
        "    elif (y_label==6):\n",
        "      processed_y.append([6])\n",
        "    elif (y_label==7):\n",
        "      processed_y.append([7])\n",
        "    elif (y_label==8):\n",
        "      processed_y.append([8])\n",
        "    else:\n",
        "      processed_y.append([9])\n",
        "      \n",
        "  return np.array(np_utils.to_categorical(np.array(processed_y),10))\n",
        "from sklearn.cluster import KMeans\n",
        "def LogisticRegression(trainData,trainResult,validData,validResult,testData,testResult,n_classes=10,batch_size=1000,learning_rate=0.1,m=100):\n",
        "  \n",
        "  trainResult=encodeOutput(trainResult)\n",
        "  validResult=encodeOutput(validResult)\n",
        "  testResultInt=testResult\n",
        "  testResult=encodeOutput(testResult)\n",
        "  validation_percent=0\n",
        "  train_percent=0.9\n",
        "  IsSynthetic=False\n",
        "  trainTranspose=np.transpose(trainData)\n",
        "  kmeans=KMeans(m,random_state=0).fit(trainData)\n",
        "  means=np.array(kmeans.cluster_centers_)\n",
        "  variance_mat     = GenerateBigSigma(trainTranspose, means,100 ,IsSynthetic) # generating variance for each input feature\n",
        "  train_phi = GetPhiMatrix(trainTranspose, means, variance_mat, 100)\n",
        "  print(train_phi.shape)\n",
        "\n",
        "  validTranspose=np.transpose(validData)\n",
        "  kmeans=KMeans(m,random_state=0).fit(validData)\n",
        "  means=np.array(kmeans.cluster_centers_)\n",
        "  variance_mat     = GenerateBigSigma(validTranspose, means,100 ,IsSynthetic) # generating variance for each input feature\n",
        "  valid_phi = GetPhiMatrix(validTranspose, means, variance_mat, 100)\n",
        "  print(valid_phi.shape)\n",
        "\n",
        "  testTranspose=np.transpose(testData)\n",
        "  kmeans=KMeans(m,random_state=0).fit(testData)\n",
        "  means=np.array(kmeans.cluster_centers_)\n",
        "  #ClusterIndicesNumpy(7, kmeans.labels_)\n",
        "  variance_mat     = GenerateBigSigma(testTranspose, means,100 ,IsSynthetic) # generating variance for each input feature\n",
        "  test_phi = GetPhiMatrix(testTranspose, means, variance_mat, 100)\n",
        "  print(test_phi.shape)\n",
        "  result=[]\n",
        "  for cl in range(0,n_classes):\n",
        "    result.append(grad_descent(train_phi,trainResult[:,cl],valid_phi,validResult[:,cl],test_phi,testResult[:,cl],batch_size,learning_rate))  \n",
        "  predictedTestLabel=[]\n",
        "  result=np.array(result).T\n",
        "  for rslt in result:\n",
        "    rslt=list(rslt)\n",
        "    predictedTestLabel.append(rslt.index(max(rslt)))\n",
        "  return(np.array(predictedTestLabel))\n",
        "  #return(np.array(predictedTestLabel))\n",
        "  \n",
        "import math\n",
        "\n",
        "def softmax(x):\n",
        "    \n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0) \n",
        "  \n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + math.exp(-x))\n",
        "\n",
        "\n",
        "def getOutput(input_data, weights):\n",
        "  predicted_result=[]\n",
        "  for input in input_data:\n",
        "      output=0\n",
        "      for i in range(1,len(weights)):\n",
        "        output=output+weights[i]*input[i-1]\n",
        "      output=sigmoid(output)\n",
        "      predicted_result.append(output)\n",
        "  return(np.array(predicted_result))\n",
        "\n",
        "def updateWeights(input_data,target_result,predicted_result,prev_weights,learning_rate):\n",
        "  subtract=np.subtract(predicted_result,target_result)\n",
        "  weight_change=np.dot(subtract,input_data)\n",
        "  weight_change/=(input_data.shape[1])\n",
        "  weight_change*=learning_rate\n",
        "  new_weights=np.subtract(prev_weights,weight_change)\n",
        "  return new_weights\n",
        "\n",
        "def loss(target_result,predicted_result):\n",
        "  loss=-target_result*np.log(predicted_result)-(1-target_result)*np.log(1-predicted_result)\n",
        "  loss=loss.sum()/target_result.shape[0]\n",
        "  return loss\n",
        "  \n",
        "def grad_descent(train_phi,trainResult,validation_phi,validationResult,test_phi,testResult,batch_size,learning_rate):\n",
        "  W=np.zeros(train_phi.shape[1])\n",
        "  i=0\n",
        "  loss_arr=[]\n",
        "  while i+batch_size <= train_phi.shape[0]:\n",
        "    input_data=train_phi[i:i+batch_size]\n",
        "    target_result=trainResult[i:i+batch_size]\n",
        "    i=i+batch_size\n",
        "    new_lr=learning_rate-(1/i)\n",
        "    if(new_lr<=0):\n",
        "      new_lr=learning_rate\n",
        "    predicted_result=np.round(getOutput(input_data,W))\n",
        "    validation_predictedResult=np.round(getOutput(validation_phi,W))\n",
        "    W=updateWeights(input_data,target_result,predicted_result,W,learning_rate)\n",
        "    loss_arr.append(loss(validationResult,validation_predictedResult))\n",
        "  test_predictedResult=(getOutput(test_phi,W))\n",
        "  loss1=loss(testResult,test_predictedResult)\n",
        "  return(test_predictedResult)   \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "0o1RQketM9bd",
        "colab_type": "code",
        "outputId": "b6b5a9fa-b366-4b44-a3ad-2bb5ff9146ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix  \n",
        "#0\n",
        "batch_size=1000\n",
        "learning_rate=0.1\n",
        "m=100\n",
        "n_classes=10\n",
        "testPrediction_lr=LogisticRegression(trainData,trainResult,validData,validResult,testData,testResult,n_classes,batch_size,learning_rate,m)\n",
        "confusion_matrix(testResultInt,testPrediction_lr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 100)\n",
            "(500, 100)\n",
            "(500, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:204: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:204: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0, 38,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0, 66,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0, 51,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0, 50,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0, 45,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0, 47,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0, 54,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0, 52,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0, 50,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0, 47,  0,  0,  0,  0,  0,  0,  0,  0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "Nx5k-GNvj9Yx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load USPS on Python 3.x"
      ]
    },
    {
      "metadata": {
        "id": "Wc5-EJtyCb6Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"USPSdata.zip\", 'r')\n",
        "zip_ref.extractall(\"USPSData\")\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_1HNCpafj9Yz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SSSbTr1-j9Y4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "USPSMat  = []\n",
        "USPSTar  = []\n",
        "curPath  = 'USPSData/USPSdata/Numerals'\n",
        "savedImg = []\n",
        "\n",
        "for j in range(0,10):\n",
        "    curFolderPath = curPath + '/' + str(j)\n",
        "    imgs =  os.listdir(curFolderPath)\n",
        "    for img in imgs:\n",
        "        curImg = curFolderPath + '/' + img\n",
        "        if curImg[-3:] == 'png':\n",
        "            img = Image.open(curImg,'r')\n",
        "            img = img.resize((28, 28))\n",
        "            savedImg = img\n",
        "            imgdata = (255-np.array(img.getdata()))/255\n",
        "            USPSMat.append(imgdata)\n",
        "            USPSTar.append(j)\n",
        "USPSMat,USPSTar=np.array(USPSMat),np.array(USPSTar)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3-VGTkDVC5ee",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "USPSMat,USPSTar=np.array(USPSMat),np.array(USPSTar)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Js2PwIlZ9XFO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Neural networks"
      ]
    },
    {
      "metadata": {
        "id": "9cAUgAz9GorN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Neural Network\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6nMPdqSy9V8U",
        "colab_type": "code",
        "outputId": "5f02168b-51a9-435d-8ecf-fa5b3fd52e92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix  \n",
        "# Neural Network\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "num_classes=10\n",
        "image_vector_size=28*28\n",
        "x_train = x_train.reshape(x_train.shape[0], image_vector_size)\n",
        "x_test = x_test.reshape(x_test.shape[0], image_vector_size)\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "image_size = 784 \n",
        "model = Sequential()\n",
        "model.add(Dense(units=32, activation='sigmoid', input_shape=(image_size,)))\n",
        "model.add(Dense(units=num_classes, activation='softmax'))\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, batch_size=128, epochs=10,\n",
        "verbose=False,validation_split=.1)\n",
        "loss,accuracy = model.evaluate(x_test, y_test, verbose=False) \n",
        "\n",
        "testPrediction=np.array(model.predict(x_test))\n",
        "testPrediction_nn=[]\n",
        "for i in range(testPrediction.shape[0]):\n",
        "  testPrediction_nn.append(testPrediction[i].argmax())\n",
        "\n",
        "y_test=np.array(y_test)\n",
        "y_test1=[]\n",
        "for i in range(y_test.shape[0]):\n",
        "  y_test1.append(y_test[i].argmax())\n",
        "  \n",
        "USPStestPrediction=np.array(model.predict(USPSMat))\n",
        "USPStestPrediction_nn=[]\n",
        "for i in range(USPStestPrediction.shape[0]):\n",
        "  USPStestPrediction_nn.append(USPStestPrediction[i].argmax())\n",
        "\n",
        "print(confusion_matrix(y_test1,testPrediction_nn))\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test1, testPrediction_nn))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 959    0    5    2    1    3    5    1    4    0]\n",
            " [   0 1106    3    5    0    1    3    1   16    0]\n",
            " [  17    2  919   15   23    1   11   15   28    1]\n",
            " [   5    0   23  918    1   29    1   15   15    3]\n",
            " [   1    3    2    1  924    1   13    1    4   32]\n",
            " [  18    4    5   63   14  723   22    8   25   10]\n",
            " [  25    4    6    1   20   12  884    1    5    0]\n",
            " [   6   10   26    4   10    2    0  946    2   22]\n",
            " [  11    8   16   30   12   18   10   16  846    7]\n",
            " [  14    6    4   16   47    8    0   28    5  881]]\n",
            "Accuracy: 0.9106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YOuY2CeWJ21P",
        "colab_type": "code",
        "outputId": "a65e8d53-e65e-4824-922f-0e2e5fdc2e18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(USPSTar,USPStestPrediction_nn))\n",
        "print(\"Accuracy:\",metrics.accuracy_score(USPSTar, USPStestPrediction_nn))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 183    4  568  112  116  494   81  109   10  323]\n",
            " [   4  162  622  623   68  192   21  273    8   27]\n",
            " [   8    6 1403  303   18  173   29   36   13   10]\n",
            " [   3    1  151 1579    2  204   12   29    4   15]\n",
            " [   2   40  160  157  771  267   19  257   51  276]\n",
            " [   7    7  261  328    7 1237   77   61    2   13]\n",
            " [  17    4  567  150   57  530  610   13    3   49]\n",
            " [   1   36  519  897   10  212    5  278   16   26]\n",
            " [   5   10  343  397   50  883   74   71   71   96]\n",
            " [   3   81  226  655   59  157   10  488  113  208]]\n",
            "Accuracy: 0.32511625581279063\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7cgPzemOGggo",
        "colab_type": "code",
        "outputId": "d3c04e53-aedc-414a-a86d-f333570d8114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3633065573692322\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cXXZaoJQ3_uh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Boosting\n",
        "\n",
        "Boosting is a technique based on ensemble. It learns on multiple algorithms and weighs its samples which are wrongly classified in the previous algorithm."
      ]
    },
    {
      "metadata": {
        "id": "1YDgkTfm4CE_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix  \n",
        "from sklearn import metrics\n",
        "from sklearn import cross_validation\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import pandas as pd\n",
        "RandomForest=RandomForestClassifier()\n",
        "kernel='rbf'\n",
        "SVM=SVC(kernel='rbf')\n",
        "\n",
        "#testPrediction_lr=LogisticRegression\n",
        "for clf,label in zip([RandomForest,SVM],[\"Random Forest Classifier\",\"Support Vector Classifier\"]):\n",
        "  accuracy=cross_validation.cross_val_score(clf,Data,Result,cv=5,scoring='accuracy')\n",
        "  print(\"Accuracy for \",label,\": \",accuracy.mean())\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NcFNRpR-LVB5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.base import ClassifierMixin\n",
        "import numpy as np\n",
        "import operator\n",
        "\n",
        "class EnsembleClassifier(BaseEstimator,ClassifierMixin):\n",
        "  def __init__(self,clfs,weights=None):\n",
        "    self.clfs=clfs\n",
        "    self.weights=weights\n",
        "  \n",
        "  def fit(self, train_data, train_result):\n",
        "    for clf in self.clfs:\n",
        "      clf.fit(train_data,train_result)\n",
        "  \n",
        "  def predict(self, train_data):\n",
        "    \n",
        "    self.prediction=np.asarray([clf.predict(train_data) for clf in self.clfs])\n",
        "    if self.weights: # if initial weights are given in the attributes while constructing the object of EnsembleClassifier\n",
        "      #predict_proba here returns average probabilty for each class for each base classifier according to the weights.\n",
        "      avg=self.predict_proba(train_data)\n",
        "      #apply_along_axis applies a function across an array. apply_along_axis(function, axis, array_name)\n",
        "      #lamda is for defining a function, which returns a result for expression of x, eg.g lambda x: x**3 returns cube of x\n",
        "      #enumerate function keep a counter or numbers the elements of an array.\n",
        "      #Finds the classifier which has the best accuracy for the particular class.\n",
        "      maj=np.apply_along_axis(lambda x:max(enumerate(x),key=operator.itemgetter(1))[0],axis=1,arr=avg)\n",
        "    else:\n",
        "      \n",
        "      maj=np.asarray([np.argmax(np.bitcount(self.prediction[:,c])) for c in range(self.prediction.shape[1])])\n",
        "    return maj\n",
        "  \n",
        "  def predict_proba(self,train_data):\n",
        "    #predicts probability of correct prediction for each class. eg. 0.8 for class 1(the model predicts class 1 with 80% correctioness), 0.3 for class 2.. and so on\n",
        "    #predict_proba returns an array of 1 x n_classes. with probability for each classifier\n",
        "    self.probabs=[clf.predict_proba(train_data) for clf in self.clfs]\n",
        "    avg=np.average(self.probabs,axis=0,weights=self.weights)\n",
        "    return avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aCKxL5U8Ot2L",
        "colab_type": "code",
        "outputId": "c1165cbe-776a-4a1a-ca2f-f3089b2d6b2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn import cross_validation\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(123)\n",
        "\n",
        "clf1 = LogisticRegression()\n",
        "clf2 = RandomForestClassifier(n_estimators=100)\n",
        "clf3 = SVC(probability=True)\n",
        "\n",
        "eclf1 = EnsembleClassifier(clfs=[clf1, clf2], weights=[1,1])\n",
        "scores = cross_validation.cross_val_score(eclf1, smallTrainData, smallTrainResult, cv=5, scoring='accuracy')\n",
        "print(\"Accuracy of the ensemble classifier:\",scores.mean())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the ensemble classifier: 0.9126014257547779\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "baOmCKrvqoXm",
        "colab_type": "code",
        "outputId": "4c0247ac-6e83-4e49-b7b0-ed61b42e356c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "eclf1 = VotingClassifier(estimators=[ ('lr', clf1), ('rf', clf2), ('svc', clf3)], voting='soft')\n",
        "eclf1 = eclf1.fit(smallTrainData, smallTrainResult)\n",
        "smallTestPrediction=eclf1.predict(smallTestData)\n",
        "print(confusion_matrix(smallTestResult,smallTestPrediction))\n",
        "print(\"Accuracy:\",accuracy_score(smallTestResult, smallTestPrediction))\n",
        "\n",
        "USPStestPrediction_eclf1=eclf1.predict(USPSMat)\n",
        "print(confusion_matrix(USPSTar,USPStestPrediction_eclf1))\n",
        "print(\"Accuracy:\",accuracy_score(USPSTar,USPStestPrediction_eclf1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[36  0  1  0  0  0  1  0  0  0]\n",
            " [ 0 63  0  0  0  0  0  0  3  0]\n",
            " [ 0  0 47  0  2  0  0  1  1  0]\n",
            " [ 0  0  0 43  0  1  1  0  5  0]\n",
            " [ 0  0  0  0 42  0  0  0  0  3]\n",
            " [ 0  1  0  3  0 40  0  0  1  2]\n",
            " [ 0  0  0  0  1  1 52  0  0  0]\n",
            " [ 0  1  1  0  2  0  0 47  1  0]\n",
            " [ 0  0  0  3  0  2  0  0 44  1]\n",
            " [ 0  0  0  4  2  0  0  0  1 40]]\n",
            "Accuracy: 0.908\n",
            "[[ 535    3  410   55  283  191   66  275   27  155]\n",
            " [  56  451  309  199  190   98   25  623   43    6]\n",
            " [  97   37 1232  123   56  206   99  108   33    8]\n",
            " [  56   13  208 1099   17  434    4   91   53   25]\n",
            " [  20   95   92   39  989  230   21  349   98   67]\n",
            " [  74   19  296  169   30 1215   65  103   21    8]\n",
            " [ 212   13  504   88   94  318  671   48   24   28]\n",
            " [ 105  294  357  237   73  279   11  562   53   29]\n",
            " [ 182   46  154  279  131  724   80  112  259   33]\n",
            " [  21  168  181  379  133  112    8  700  191  107]]\n",
            "Accuracy: 0.3560178008900445\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "fg9dSMOlAOZw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix  \n",
        "#LogisticRegression\n",
        "print(\"-------Ense\")\n",
        "batch_size=1000\n",
        "learning_rate=0.1\n",
        "m=100\n",
        "n_classes=10\n",
        "testPrediction_lr=LogisticRegression(trainData,trainResult,validData,validResult,testData,testResult,n_classes,batch_size,learning_rate,m)\n",
        "print(confusion_matrix(testResultInt,testPrediction_lr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oeC_IPKr-rft",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Max Voting"
      ]
    },
    {
      "metadata": {
        "id": "H500CcXEDxjb",
        "colab_type": "code",
        "outputId": "e7a5aabe-5ece-4554-c0be-6108feeadfe8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from statistics import mean,mode\n",
        "\n",
        "import statistics\n",
        "def mode(list1):\n",
        "    list_table = statistics._counts(list1)\n",
        "    len_table = len(list_table)\n",
        "\n",
        "    if len_table == 1:\n",
        "        max_mode = statistics.mode(list1)\n",
        "    else:\n",
        "        new_list = []\n",
        "        for i in range(len_table):\n",
        "            new_list.append(list_table[i][0])\n",
        "        max_mode = max(new_list) # use the max value here\n",
        "    return max_mode\n",
        "  \n",
        "testPrediction_rf.astype(int)\n",
        "testPrediction_svm.astype(int)\n",
        "testPrediction_nn=np.array(testPrediction_nn)\n",
        "testPrediction_nn.astype(int)\n",
        "#testPrediction_lr.astype(int)\n",
        "testPrediction=[]\n",
        "for i in range(len(testPrediction_rf)):  \n",
        "  list=[int(testPrediction_rf[i]),int(testPrediction_svm[i]),int(testPrediction_nn[i])]\n",
        "  temp=int(mode(list))\n",
        "  if temp:\n",
        "    testPrediction.append(temp)\n",
        "  else:\n",
        "    testPrediction.append(int(mean(list)))\n",
        "    #print(testPrediction)\n",
        "print(\"Accuracy:\",accuracy_score(smallTestResult, testPrediction))\n",
        "print(confusion_matrix(smallTestResult,testPrediction))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.734\n",
            "[[15  8 11  3  0  0  1  0  0  0]\n",
            " [ 0 64  0  0  0  0  0  0  2  0]\n",
            " [ 0  2 31  1  3  0  2  6  3  3]\n",
            " [ 0  2  0 33  3  2  0  5  1  4]\n",
            " [ 0  0  0  0 35  0  0  1  0  9]\n",
            " [ 0  5  0  3  1 13  4  1  9 11]\n",
            " [ 0  0  0  0  0  1 48  1  2  2]\n",
            " [ 0  2  1  0  0  1  0 48  0  0]\n",
            " [ 0  1  0  2  0  1  0  0 37  9]\n",
            " [ 0  0  0  0  1  1  0  1  1 43]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nnuk2x_77z7z",
        "colab_type": "code",
        "outputId": "4c373b76-f675-4dd7-d094-c8480466b7b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "USPStestPrediction_rf.astype(int)\n",
        "USPStestPrediction_svm.astype(int)\n",
        "USPStestPrediction_nn=np.array(USPStestPrediction_nn)\n",
        "USPStestPrediction_nn.astype(int)\n",
        "USPStestPrediction_eclf1=np.array(USPStestPrediction_eclf1)\n",
        "USPStestPrediction_eclf1.astype(int)\n",
        "#testPrediction_lr.astype(int)\n",
        "USPStestPrediction=[]\n",
        "for i in range(len(USPStestPrediction_rf)):  \n",
        "  list=[int(USPStestPrediction_rf[i]),int(USPStestPrediction_svm[i]),int(USPStestPrediction_nn[i]),int(USPStestPrediction_eclf1[i])]\n",
        "  temp=int(mode(list))\n",
        "  if temp:\n",
        "    USPStestPrediction.append(temp)\n",
        "  else:\n",
        "    USPStestPrediction.append(int(mean(list)))\n",
        "    #print(testPrediction)\n",
        "print(\"Accuracy:\",accuracy_score(USPSTar, USPStestPrediction))\n",
        "print(confusion_matrix(USPSTar,USPStestPrediction))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.35381769088454423\n",
            "[[ 245  191  444   55  248  242   87  224   21  243]\n",
            " [  10  516  273  154  113  157   29  716   26    6]\n",
            " [  48   73 1252  113   41  214   84  133   32    9]\n",
            " [  28   29  138 1154   13  442    8  116   44   28]\n",
            " [   2  141   84   20  943  246   20  332   80  132]\n",
            " [  29   48  233  133   17 1335   63  113   19   10]\n",
            " [  20  150  469   78   82  400  678   73   15   35]\n",
            " [  13  393  354  209   40  348   16  555   35   37]\n",
            " [  12   99  203  218   88  891   91   96  242   60]\n",
            " [   3  233  189  325  115  143    8  653  175  156]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HI7jaBbd85wr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Citation\n",
        "https://sebastianraschka.com/Articles/2014_ensemble_classifier.html"
      ]
    }
  ]
}